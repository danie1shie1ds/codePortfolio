{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"010fbc73-0d91-40a2-8e1f-bfc645963a7e","_cell_guid":"fce771f2-070f-4122-ad6d-8bdb4ef7875a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-20T23:21:12.098536Z","iopub.execute_input":"2023-09-20T23:21:12.099469Z","iopub.status.idle":"2023-09-20T23:21:12.108959Z","shell.execute_reply.started":"2023-09-20T23:21:12.099431Z","shell.execute_reply":"2023-09-20T23:21:12.107711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections, math, random, sys","metadata":{"_uuid":"afdc5e69-550c-4694-8880-809d6245c00c","_cell_guid":"39bc184b-dfa0-41eb-9893-2514f57f4325","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-20T23:21:12.111406Z","iopub.execute_input":"2023-09-20T23:21:12.111832Z","iopub.status.idle":"2023-09-20T23:21:12.118552Z","shell.execute_reply.started":"2023-09-20T23:21:12.111794Z","shell.execute_reply":"2023-09-20T23:21:12.117316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \ntorch.set_default_device('cpu')\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.120364Z","iopub.execute_input":"2023-09-20T23:21:12.121152Z","iopub.status.idle":"2023-09-20T23:21:12.132810Z","shell.execute_reply.started":"2023-09-20T23:21:12.121104Z","shell.execute_reply":"2023-09-20T23:21:12.131437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('/kaggle/input/nus-sms-corpus/')\nprint(sys.path)\nfrom utils import *\nprint(sys.path)","metadata":{"_uuid":"60b0b5db-f43c-461b-bad2-c8bbd1c7d3a6","_cell_guid":"1bb56070-98fd-4b22-9372-ff7f39fb2231","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-20T23:21:12.135297Z","iopub.execute_input":"2023-09-20T23:21:12.135728Z","iopub.status.idle":"2023-09-20T23:21:12.147814Z","shell.execute_reply.started":"2023-09-20T23:21:12.135697Z","shell.execute_reply":"2023-09-20T23:21:12.146977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindata = read_mono('/kaggle/input/nus-sms-corpus/data/small', delim='')\ndevdata = read_mono('/kaggle/input/nus-sms-corpus/data/dev', delim='')\ntestdata = read_mono('/kaggle/input/nus-sms-corpus/data/test', delim='')\n\nvocab = Vocab()\nfor words in traindata:\n    vocab |= words","metadata":{"_uuid":"624008a9-bc34-4b6c-afd2-0a373ce814f1","_cell_guid":"c3b46a48-da58-4be2-9312-7daae469ec9e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-20T23:21:12.149447Z","iopub.execute_input":"2023-09-20T23:21:12.149791Z","iopub.status.idle":"2023-09-20T23:21:12.443973Z","shell.execute_reply.started":"2023-09-20T23:21:12.149764Z","shell.execute_reply":"2023-09-20T23:21:12.442681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # create model that numberizes each word in the dataset\n# class CustomDataset(Dataset):\n#     def __init__(self, data, vocab):\n#         self.data = data\n#         self.vocab = vocab\n        \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, idx):\n#         return [self.vocab.numberize(word) for word in self.data[idx]]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.445381Z","iopub.execute_input":"2023-09-20T23:21:12.445711Z","iopub.status.idle":"2023-09-20T23:21:12.450757Z","shell.execute_reply.started":"2023-09-20T23:21:12.445682Z","shell.execute_reply":"2023-09-20T23:21:12.449632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, voc_dim, hidden_dim):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.voc_dim = voc_dim\n        self.lstm = nn.LSTMCell(voc_dim, hidden_dim)\n        self.linear = nn.Linear(hidden_dim, voc_dim)\n        \n    \n    def forward(self, x, hc):\n        # LSTMCell(x, (h, c)) returns (h', c')\n        h, c = self.lstm(x, hc)\n        y = nn.functional.log_softmax(self.linear(h), dim=1)\n        return y, (h, c)\n    \nhidden_dim = 128\n\n        \n    \n        ","metadata":{"_uuid":"cbf28626-1391-49cb-9bd8-662433e48bfd","_cell_guid":"182ae6b4-08d6-48a0-9442-8f4154e08162","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-20T23:21:12.452383Z","iopub.execute_input":"2023-09-20T23:21:12.452722Z","iopub.status.idle":"2023-09-20T23:21:12.465840Z","shell.execute_reply.started":"2023-09-20T23:21:12.452694Z","shell.execute_reply":"2023-09-20T23:21:12.464955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train that jawn\ndef train(model, traindata, vocab, optimizer, criterion, num_epochs=10):\n    prev_dev_acc = None\n    for epoch in range(num_epochs):\n        random.shuffle(traindata) #important\n        train_loss = 0.0\n        for line in traindata:\n            optimizer.zero_grad()\n            # inputs are 0->n-1 bc they predict the next char\n            inputs = torch.tensor([vocab.numberize(word) for word in line[:-1]])\n            #use one_hot to avoid embeddings bc I don't know what they are\n            inputs = torch.nn.functional.one_hot(inputs, len(vocab)).float()\n            #targets are 1->n bc they are the ones being predicted\n            targets = torch.tensor([vocab.numberize(word) for word in line[1:]])\n            # init hidden and cell states to 0 tensors from 2.17\n            # matrix of len(line) - BOS by all of the hidden states\n            h, c = torch.zeros(len(line)-1, model.hidden_dim), torch.zeros(len(line)-1, model.hidden_dim)\n            # this is how we get the \n            output, hidden = model(inputs, (h, c))\n            # compute loss \n            loss = criterion(output.view(-1, len(vocab)), targets)\n            \n            # reset gradients and update parameters/train loss\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_loss += loss.item()\n        \n        dev_acc = evaluate(model, devdata, vocab)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n        \n        #if dev acc decreases, halve learning rate\n        if prev_dev_acc is not None and dev_acc <= prev_dev_acc:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= 0.5\n            print(f\"lr={optimizer.param_groups[0]['lr']}\")\n            \n            # break if learning rate goes below threshold\n            if optimizer.param_groups[0]['lr'] < .000001:\n                break\n            \n            prev_dev_acc = dev_acc\n            \n                \n\n    # save the trained model\n    torch.save((list(model.parameters()), vocab), 'model.small')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.467384Z","iopub.execute_input":"2023-09-20T23:21:12.468263Z","iopub.status.idle":"2023-09-20T23:21:12.486606Z","shell.execute_reply.started":"2023-09-20T23:21:12.468219Z","shell.execute_reply":"2023-09-20T23:21:12.485455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate function\ndef evaluate(model, data, vocab):\n    with torch.no_grad():\n        num_correct = 0\n        total = 0\n        \n        for l in data:\n            inputs = [vocab.numberize(word) for word in l[:-1]]\n            inputs = torch.nn.functional.one_hot(torch.tensor(inputs), len(vocab)).float()\n            targets = [vocab.numberize(word) for word in l[1:]]\n            targets = torch.tensor(targets)\n            \n            h, c = torch.zeros(len(l)-1, model.hidden_dim), torch.zeros(len(l)-1, model.hidden_dim)\n            # this is how we get the outputs and check if they're correct\n            output, hidden = model(inputs, (h, c))\n            hidden, predicted = output.max(1)\n            \n            num_correct += (predicted == targets).sum().item()\n            total += len(targets)\n            \n        acc = num_correct / total\n            \n    return acc\n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.490266Z","iopub.execute_input":"2023-09-20T23:21:12.490638Z","iopub.status.idle":"2023-09-20T23:21:12.505235Z","shell.execute_reply.started":"2023-09-20T23:21:12.490604Z","shell.execute_reply":"2023-09-20T23:21:12.504259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the model\nmodel = LSTMModel(len(vocab), hidden_dim)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.506329Z","iopub.execute_input":"2023-09-20T23:21:12.506676Z","iopub.status.idle":"2023-09-20T23:21:12.521720Z","shell.execute_reply.started":"2023-09-20T23:21:12.506647Z","shell.execute_reply":"2023-09-20T23:21:12.520728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.523299Z","iopub.execute_input":"2023-09-20T23:21:12.524093Z","iopub.status.idle":"2023-09-20T23:21:12.533408Z","shell.execute_reply.started":"2023-09-20T23:21:12.524053Z","shell.execute_reply":"2023-09-20T23:21:12.532453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained = train(model, traindata, vocab, optimizer, criterion)\nacc = evaluate(trained, devdata, vocab)\nprint(f\"Final Dev Accuracy: {acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T23:21:12.534661Z","iopub.execute_input":"2023-09-20T23:21:12.535041Z","iopub.status.idle":"2023-09-20T23:28:15.857138Z","shell.execute_reply.started":"2023-09-20T23:21:12.535012Z","shell.execute_reply":"2023-09-20T23:28:15.855636Z"},"trusted":true},"execution_count":null,"outputs":[]}]}